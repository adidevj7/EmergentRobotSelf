# /home/adi/projects/CreativeMachinesAnt/Isaac/cfg/rlg_walk_new_v0.yaml
seed: 42

params:
  algo:
    name: sac

  model:
    name: soft_actor_critic

  # AFTER
  network:
    name: soft_actor_critic
    separate: true
    # REQUIRED by this rl_games build:
    mlp:
      units: [256, 256]
      activation: elu
      d2rl: false
      initializer:
        name: default
    # (optional but harmless on many builds)
    log_std_bounds: [-20.0, 2.0]   # ‚Üê ADD THIS
    space:
      continuous:
        mu_activation: null
        sigma_activation: null



  config:
    print_stats: false
    gamma: 0.99
    critic_tau: 0.005
    batch_size: 1024
    replay_buffer_size: 1000000
    num_warmup_steps: 10000

    actor_lr: 0.0003
    critic_lr: 0.0003
    alpha_lr: 0.003
    learnable_temperature: True
    init_alpha: 0.2
    target_entropy: -8.0          # If this errors, set to -8.0 (action_dim = 8)

    grad_norm: 1.0
    bound_loss_type: regularization

    reward_shaper:
      scale_value: 1.0

    # These are patched by your script at runtime, but safe to keep as defaults:
    env_name: isaaclab
    num_actors: 36
    vecenv_type: LOCAL
    train_dir: /home/adi/projects/CreativeMachinesAnt/Isaac/logs/rl_games/ant_walk
    name: placeholder
    device: cuda

    max_env_steps: 500000
    max_epochs: 500000

    record_video: False
    record_best_video: False
    record_stats: True
